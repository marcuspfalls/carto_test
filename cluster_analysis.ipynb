{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis\n",
    "\n",
    "This notebook contains the function cluster_analysis found in cluster_analysis.py. Each block is documented below with various aspects: The research and justification behind each component and also any experience or challenges that arose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopy.distance\n",
    "import geopandas\n",
    "from shapely.geometry import Point, mapping\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that the modules were compatible with each other was a challenge. I use Numpy and Pandas on a daily basis, and have some experience with Geopy, Geopandas and Shapely. No previous experience using sci-kit, although I have studied some of the statistics and graph theory behind the techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining the data\n",
    "bqclient=bigquery.Client()\n",
    "table = bigquery.TableReference.from_string('carto-ps-bq-developers.data_test.osm_spain_pois')\n",
    "rows = bqclient.list_rows(table)\n",
    "spain_pois = rows.to_dataframe(create_bqstorage_client=True)\n",
    "spain_pois.to_csv('spain_pois.csv')\n",
    "\n",
    "# spain_pois=pd.read_csv('spain_pois.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having never used Google Cloud prior to this exercise, I initially found the interface quite difficult to begin with, but eventually I was able to set it up as required and obtain the data. I would like the opportunity to work with it further, as it seems to have huge potential.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discarding the points outside of the Madrid Region\n",
    "min_lon = -3.93455628\n",
    "min_lat = 40.25387182\n",
    "max_lon = -3.31993445\n",
    "max_lat = 40.57085727\n",
    "madrid_pois = spain_pois.loc[(spain_pois['lon'] >= min_lon) & (spain_pois['lon'] <= max_lon) &\n",
    "                             (spain_pois['lat'] >= min_lat) & (spain_pois['lat'] <= max_lat)]\n",
    "madrid_pois=madrid_pois.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working regularly with Pandas, this was straightforward. Used reset_index to allow for easier iteration later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cinemas\n",
    "madrid_cinemas = pd.read_csv('208862-7650164-ocio_salas.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumed that spain_pois.csv would contain the cinemas in Madrid as Points of Interest but I was unable to find them. I did a check for 'cinema' and 'cine' under all of the columns but could find nothing. The closest I could find was 'theatre' under amenity - but a check on google maps showed that these were stage theatres and not cinemas. \n",
    "The .csv file was obtained from https://datos.madrid.es/portal/site/egob/. Unfortunately it only contains the cinemas within the municipality of Madrid and not all of the region. The website of the Communidad de Madrid did not contain this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting degrees to meters, using min_lon and min_lat as the origin\n",
    "def lat_to_m(mlat,lat,mlon):\n",
    "    return geopy.distance.distance((mlon, lat), (mlon, mlat)).km\n",
    "def lon_to_m(mlon,lon,mlat):\n",
    "    return geopy.distance.distance((lon, mlat), (mlon, mlat)).km\n",
    "vlat_to_m = np.vectorize(lat_to_m)\n",
    "vlon_to_m = np.vectorize(lon_to_m)\n",
    "\n",
    "madrid_pois['y_metres'] = vlat_to_m(min_lat, np.array(madrid_pois['lat']), min_lon)\n",
    "madrid_pois['x_metres'] = vlon_to_m(min_lon, np.array(madrid_pois['lon']), min_lat)\n",
    "madrid_cinemas['y_metres'] = vlat_to_m(min_lat, np.array(madrid_cinemas['LATITUD']), min_lon)\n",
    "madrid_cinemas['x_metres'] = vlon_to_m(min_lon, np.array(madrid_cinemas['LONGITUD']), min_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying the buffer function (below), I noticed that most points were being filtered as I was using 0.15 degrees, not kilometres. I decided to add extra columns to both the poi and cinema dataframes that are their horizontal and vertical displacements from an origin, of which I used (min_lat, min_lon). Given there are >90000 data points I opted for vectorizing these functions over a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the region of Madrid that is less than 0.15kn from a cinema\n",
    "cinema_locs = geopandas.GeoSeries([Point(madrid_cinemas['y_metres'][i], madrid_cinemas['x_metres'][i]) for i in range(madrid_cinemas.shape[0])])\n",
    "cinema_buffer = cinema_locs.buffer(0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used the y_metres and x_meters as points instead of coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard the pois near cinemas\n",
    "madrid_pois['near_cinema'] = [0 for i in range(madrid_pois.shape[0])]\n",
    "\n",
    "def in_buffer(p):\n",
    "    p = Point(madrid_pois['y_metres'][p], madrid_pois['x_metres'][p])\n",
    "    for buff in cinema_buffer:\n",
    "        if buff.contains(p):\n",
    "            return 1\n",
    "\n",
    "vin_buffer = np.vectorize(in_buffer)\n",
    "madrid_pois['near_cinema'] = vin_buffer(range(madrid_pois.shape[0]))\n",
    "madrid_pois_nocinema = madrid_pois.loc[madrid_pois['near_cinema']!=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized to allow it to run faster. It takes each point as input and loops it through each 150m radius of the cinemas. The loop stops early if one has been found. Opted for marking each with a 1 and filtering at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a 20000 sample\n",
    "madrid_pois_nocinema = madrid_pois_nocinema.sample(n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took a 20000 sample to perform the analysis. Doing the cluster analysis on all points on my machine caused a memory error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the cluster analysis\n",
    "X = np.zeros((madrid_pois_nocinema.shape[0],2))\n",
    "X[:,0],X[:,1]  = np.array(madrid_pois_nocinema['lat']), np.array(madrid_pois_nocinema['lon'])\n",
    "model = SpectralClustering(n_clusters=5)\n",
    "yhat = model.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New to this kind of analysis so did some research on the possible options. I found this article [https://machinelearningmastery.com/clustering-algorithms-with-python/](https://machinelearningmastery.com/clustering-algorithms-with-python/) that gave 10 and I read though it and decided that the two most worth considering were the last two - Spectral Clustering and Gaussian Mixture, mainly because the main hyperparameter was the number of clusters. I first opted for Gaussian mixture first, but after plotting it did not produce a very suitable result on the map. So I opted for Spectral Clustering, which produced a more sensible end result. One advantage of Gaussian mixture is that it uses less memory than Spectral Clustering. I had also considered implementing a genetic algorithm using graph theory and distances as a cost function, but opted not to due to time restraints. Also the size of the problem (90000) was simply too large for the computing resources available to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'lat':np.array(madrid_pois_nocinema['lat']),\n",
    "                    'lon':np.array(madrid_pois_nocinema['lon']),\n",
    "                    'point_id':np.array(madrid_pois_nocinema['id']),\n",
    "                    'cluster_id':yhat})\n",
    "\n",
    "out.to_csv('madrid_poi_clusters.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved to .csv. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
